\section{Preliminaries}\label{sec:prelims}

We use $[n]$ to denote the range $\{1, \dots, n\}$. For other ranges (mostly zero-indexed), we explicitly write the (inclusive) endpoints, e.g., $[0,n]$. 
Concatenation of vectors $\vec{x},\vec{y}$ is written as $\vec{x} || \vec{y}$. 
Let $\secpar$ be the security parameter.
We use the uppercase variable $X$ for the free variable of a polynomial, e.g., $f(X)$. 
We use a calligraphic font, e.g., $\mathcal{S}$ or $\mathcal{X}$, to denote sets or domains. When we apply an operation to two sets of equal size $\ell$ we mean pairwise application, e.g., $\mathcal{Z} = \mathcal{X} + \mathcal{Y}$ means $z_i = x_i + y_i~\forall{i \in [\ell]}$. \todo{empty set $\emptyset$ vs. $\perp$}
Sampling an element $x$ uniformly at random from a set $\mathcal{X}$ is written as $x \sample \mathcal{X}$. 
We use $:=$ to denote variable assignment, $y \gets \mathsf{Alg}(x)$ to assign to $y$ the output of some algorithm $\mathsf{Alg}$ on input $x$, and $y \sample \mathsf{Alg}(x)$ if the algorithm is randomized (or sometimes $\mathsf{Alg}(x) \randout y$). When we wish to be explicit about the randomness $r$ used, we write $y \gets \mathsf{Alg}(x; r)$. We use $\mathcal{D}_1 \approx_\secpar \mathcal{D}_2$ to denote that two distributions $\mathcal{D}_1, \mathcal{D}_2$ have statistical distance bounded by $\negl$.

For a non-interactive proof system $\Pi$, we write $\pi \gets \Pi.\prove(x; w)$ to show that the proving algorithm takes as input an instance $x$ and witness $w$ and outputs a proof $\pi$. Verification is written as $\Pi.\vrfy(x, \pi)$ and outputs a bit $b$. 

We distinguish the key-pairs used in a signature scheme ($\vk, \sk$ for ``verification'' and ``signing'' key, respectively) from those used in an encryption scheme ($\ek, \dk$ for ``encryption'' and ``decryption'' key, respectively). 

\subsection{Non-interactive proof systems}\label{sec:nizks}

\todo{NP-languages and relations} \noemi{A language is some subset of strings. Every NP language has a corresponding polynomially-decidable relation (i.e., decidable by a circuit $C(x,w)$ such that $\sizeof{C} \in \poly[\sizeof{x}]$) such that if $x \in \Lang, \exists w \suchthat\allowbreak (x,w) \in \Rel_\Lang$ where $w$ is polynomial in $\sizeof{x}$.}

\begin{definition}[Non-interactive proof system] A non-interactive proof system $\Pi$ for some NP language $\Lang$ is a tuple of PPT algorithms $(\setup, \prove, \vrfy)$:
    \begin{itemize}
        \item \underline{$\setup(\secparam) \rightarrow \crs$:} Given a security parameter, output a common reference string $\crs$. This algorithm might use private randomness (a trusted setup).
        \item \underline{$\prove(\crs, x, \witness) \rightarrow \pi$:} Given the $\crs$, an instance $x$, and witness $\witness$ such that $(x, \witness) \in \Rel_\Lang$, output a proof $\pi$.
        \item \underline{$\vrfy(\crs, x,\pi) \rightarrow \{0,1\}$:} Given the $\crs$ and a proof $\pi$ for the instance $x$, output a bit indicating accept or reject.
    \end{itemize}

We require (perfect) \emph{completeness}, meaning that for all $(x,\witness) \in \Rel_Lang$,
\begin{equation*}
    \Pr\left[
        \vrfy(\crs, x, \pi)=1 
        \middle| 
        \begin{array}{c}
            \crs \gets \setup(\secparam)~\land\\
            \pi \gets \prove(\crs, x, \witness)
        \end{array}
    \right] = 1.
\end{equation*}
\end{definition}

\begin{definition}[soundness] Soundness requires that for all $x \notin \Lang$, $\secpar \in \NN$, and all PPT adversaries $\adv$,
\begin{equation*}
    \Pr\left[
        \vrfy(\crs, x, \pi)=1 
        \middle| 
        \begin{array}{c}
            \crs \gets \setup(\secparam)~\land\\
            \pi \gets \adv(\crs, x)
        \end{array}
    \right] \leq \negl.
\end{equation*}
\end{definition}

We refer the reader to \cite{Thaler23,GoldreichFoC} for a formal description of other properties of proof systems (e.g., correctness, zero-knowledge). 

There are numerous definitions of succinctness adopted for proof systems in the literature. In this work, we require succinct proof systems only to have proofs which are sublinear in $\sizeof{w}$ and ``work-saving'' verification (faster than $C(x,w)$)\footnote{\url{https://a16zcrypto.com/posts/article/17-misconceptions-about-snarks/\#section--3}}:

\begin{definition}[succinct]
    We say a proof system $\Pi$ is \emph{succinct} if $\sizeof{\pi} \in o(\sizeof{\witness})$ and $\Pi.\vrfy(\crs, x, \pi)$ runs in time $o(\poly{\sizeof{x}})$.
\end{definition}

% For an instance $(x;\omega)$ of an NP language $\mathcal{R}$, we denote the public and private witness elements before and after the semicolon, respectively.

\subsection{Time-Lock Puzzles}\label{sec:tlp}

A time-lock puzzle (TLP)~\cite{RivShaWag96} consists of three efficient algorithms $\mathsf{TLP} = (\mathsf{Setup}, \mathsf{Gen}, \mathsf{Solve})$ allowing a party to ``encrypt'' a message to the future. To recover the solution, one needs to perform a computation that is believed to be inherently sequential, with a parameterizable number of steps.

\begin{definition}[Time-lock puzzle~\cite{RivShaWag96}] A time-lock puzzle scheme $\sf TLP = (\Setup, \mathsf{Gen}, \mathsf{Solve})$ for solution space $\mathcal{X}$ consists of the following three efficient algorithms:
    \begin{itemize}
        \item \underline{$\mathsf{TLP.Setup}(\secparam, \Ttime) \randout \pparam$:} The (potentially trusted) setup algorithm takes as input a security parameter $\secparam$ and a difficulty (time) parameter $\Ttime$, and outputs public parameters $\pparam$. % (usually a group $\mathbb{G}$ with $\lambda$ bits of security). 
        % Typically $\mathbb{G}$ is a group of unknown order, e.g., the group $\mathbb{Z}^{*}_N$. 
        \item \underline{$\mathsf{TLP.Gen}(\pparam, s) \randout Z$:} Given a solution $s \in \X$, the puzzle generation algorithm efficiently computes a time-lock puzzle $Z$.
        \item \underline{$\mathsf{TLP.Solve}(\pparam, Z) \rightarrow s$:} Given a TLP $Z$, the puzzle-solving algorithm requires at least $\Ttime$ sequential steps to output the solution $s$.
    \end{itemize}
\end{definition}

Informally, we say that a TLP scheme is \emph{correct} if $\mathsf{TLP.Gen}$ is efficiently computable, and $\tlp.{\sf Solve}$ always recovers the original solution $s$ to a validly constructed puzzle. A TLP scheme is \emph{secure} if $Z$ hides the solution $s$ and no adversary can compute $\mathsf{TLP.Solve}$ in fewer than $\Ttime$ steps with non-negligible probability. For the formal definitions, see~\cite{C:MalThy19}.

\paragraph{Homomorphic TLPs.}
Malavolta and Thyagarajan~\cite{C:MalThy19} introduce \emph{homomorphic} TLPs (HTLPs). An HTLP is defined with respect to a circuit class $\mathcal{C}$ and has an additional algorithm $\sf Eval$ defined as:
\begin{itemize}
    \item \underline{$\htlp.{\sf Eval}(\pparam, C, Z_1, \dots, Z_m) \rightarrow Z_*$:} Given the public parameters, a circuit $C \in \mathcal{C}$ where $C: \X^m \rightarrow \X$, and input puzzles $Z_1, \dots, Z_m$, the homomorphic evaluation algorithm outputs a puzzle $Z_*$.
\end{itemize}

Correctness requires that $\htlp.{\sf Solve}(Z_*)$
% the puzzle obtained by homomorphically applying the circuit $C$ to $m$ puzzles 
should contain the expected solution, namely $C(s_1, \dots, s_m)$, where $s_i \gets \htlp.{\sf Solve}(Z_i)$. Again, we refer the reader to \cite{C:MalThy19} for the formal definition. Moving forward, we will use $\boxplus$ for homomorphic addition and $\cdot$ for scalar multiplication of HTLPs. For the homomorphic application of a linear function $f$, we write $f(Z_1, \dots, Z_m)$.

% \noemi{The definition is slightly more complicated since it also requires the new HTLP to be solvable in the same amount of steps; let's just give an informal def and refer reader to \cite{malavolta2019homomorphic} for details.}
% For correctness, we require that the following holds for all $\secpar, \Ttime \in \mathbb{N}$, $C \in \mathcal{C}$, and $s_1, \dots, s_m \in \ZZ$:
% \[
%     \Pr\left[
%         \htlp.{\sf Solve}(\pparam, Z_*) = C(s_1, \dots, s_m) 
%         \middle| 
%         \begin{array}{c}
%             \pparam \sample \htlp.{\sf Setup}(\secparam, \Ttime)\\
%             Z_i \sample \htlp.{\sf Gen}(\pparam, s)~\forall i\in [m]~\land\\
%             Z_* \gets \htlp.{\sf Eval}(\pparam, C, Z_1, \dots, Z_m)
%         \end{array}
%     \right] = 1
% \]

% removed the below because we're not using this shorthand anymore
% Later in the paper, we may apply $\sf Eval$ to \emph{vectors} of puzzles of common size $k$ to indicate element-wise application of the function, e.g., $\{Z_1'', \dots, Z_k''\} \gets \htlp.{\sf Eval}(\pparam, f, \{Z_1, \dots, Z_k\}, \{Z_1', \dots, Z_k'\})$ means $Z_i'' \gets \htlp.{\sf Eval}(\pparam, C, Z_i, Z_i')$ for all $i \in [k]$.

\subsection{HTLP Constructions}\label{app:htlp_constructions}

Malavolta and Thyagarajan~\cite{C:MalThy19} give two HTLP constructions with linear and multiplicative homomorphisms, respectively. They require $N$ to be a \emph{strong} semiprime, i.e., $N = p \cdot q$ such that $p = 2p' + 1$ and $q = 2q' + 1$ where $p', q'$ are also prime. The linearly-homomorphic HTLP is based on Paillier encryption~\cite{C:Paillier99}, while the multiplicative homomorphism is achieved by working over the subgroup $\JJ_N \subseteq \ZZ_N^*$ of elements with Jacobi symbol $+1$. 
We recall their constructions below.
% We recall their constructions in \Cref{fig:mt19-HTLP}.

% \begin{figure}[tb]
%     \centering
%     \begin{mdframed}
    \begin{construction}[Linear HTLP~\cite{C:MalThy19}.]\label{con:paillierHTLP}
    \hfill
    \begin{itemize}
        \item \underline{$\htlp.\Setup(\secparam, \Ttime) \randout \pparam$:} Sample a strong semiprime $N$ and a generator $g \sample \ZZ_N^*$, then compute $h = g^{2^\Ttime} \mod{N} \in \ZZ_N^*$. (This can be computed efficiently using the factorization of $N$). Output $\pparam := (N, g, h)$.
        \item \underline{$\htlp.{\sf Gen}(\pparam, s; r) \rightarrow Z$:} Given a value $s \in \ZZ_N$, use randomness $r \in \ZZ_{N^2}$ to compute and output
            $$Z := (g^r \mod{N},\ h^{r \cdot N} \cdot (1+N)^s \mod{N^2}) \in \JJ_N \times \ZZ_{N^2}^*$$
        \item \underline{$\htlp.\Open(\pparam, Z, r) \rightarrow s$:} Parse $Z := (u,v)$ and compute $w := u^{2^T} \mod{N} \allowbreak= h^r$ via repeated squaring. Output $s := \frac{(v/w^N \mod{N^2})- 1}{N}$.
        \item \underline{$\htlp.{\sf Eval}(\pparam, f, Z_1, Z_2) \rightarrow Z$:} To evaluate a linear function $f(x_1, x_2) = b + a_1 x_1 + a_2 x_2$ homomorphically on puzzles $Z_1 := (u_1, v_1)$ and $Z_2 := (u_2, v_2)$, return
        $$Z = (u_1^{a_1} \cdot u_2^{a_2} \mod{N}, v_1^{a_1} \cdot v_2^{a_2} \cdot (1+N)^b \mod{N^2}).$$
    \end{itemize}
    \end{construction}

\noindent Correctness holds because for all $s \in \ZZ_N$ and $Z = (u,v) \gets \htlp.{\sf Gen}(\pparam, s)$,
\begin{equation}\label{eq:correctness1}
\htlp.\Open(\pparam, Z) = \frac{(v/(h^R)^N \mod{N^2})-1}{N} = \frac{((1+N)^s) - 1}{N} = s
\end{equation}
since $(1+N)^x = 1+Nx \mod{N^2}$.
Correctness of the homomorphism follows since for all linear functions $f(x_1, x_2) = b + a_1 x_1 + a_2 x_2$ and all $Z_i = (u_i, v_i) \in {\sf Im}(\htlp.{\sf Gen}(\pparam, s_i; r_i))$ for $i \in \{1,2\}$,\footnote{For space and clarity we drop the moduli and assume that we are working in the appropriate ring in each coordinate (namely $\mathbb{Z}_N$ and $\mathbb{Z}_{N^2}$, respectively).}
% \begin{equation}\label{eq:correctness2}
% \begin{aligned}
\begin{align*}
&\htlp\rlap{$.{\sf Eval}(\pparam, f, Z_1, Z_2) = (u_1^{a_1} \cdot u_2^{a_2}, (1+N)^b \cdot v_1^{a_1} \cdot v_2^{a_2})$} && \\
&= (g^{r_1 a_1} \cdot g^{r_2 a_2}, &&(1+N)^b \cdot h^{r_1 N a_1} \cdot (1+N)^{s_1 a_1} \cdot h^{r_2 N a_2} \cdot (1+N)^{s_2 a_2})\\
&= (g^{r_1 a_1 + r_2 a_2},         &&h^{(r_1 a_1 + r_2 a_2) \cdot N} \cdot (1+N)^{b + s_1 a_1 + s_2 a_2}) \\
&\rlap{$= \htlp.{\sf Gen}(\pparam, f(s_1, s_2); r_1 a_1 + r_2 a_2)$} &&
\end{align*}
% \end{aligned}
% \end{equation}
which opens to $f(s_1, s_2)$ by \cref{eq:correctness1}.


    \begin{construction}[Multiplicative HTLP~\cite{C:MalThy19}.]\label{con:multHTLP}
    \hfill
    \begin{itemize}
        \item \underline{$\htlp.\Setup(\secparam, \Ttime) \randout \pparam$:} Same as \Cref{con:paillierHTLP}.
        \item \underline{$\htlp.{\sf Gen}(\pparam, s; r) \rightarrow Z$:} Given a value $s \in \JJ_N$, use randomness $r \in \ZZ_{N^2}$ to compute and output
            $$Z := (g^r \mod{N},\ h^r \cdot s \mod{N}) \in \ZZ_N^* \times \ZZ_N^*$$
        \item \underline{$\htlp.\Open(\pparam, Z, r) \rightarrow s$:} Parse $Z := (u,v)$ and compute $w := u^{2^T} \mod{N} \allowbreak= h^r$ via repeated squaring. Output $s := v/w$.
        \item \underline{$\htlp.{\sf Eval}(\pparam, f, Z_1, Z_2) \rightarrow Z$:} To evaluate a multiplicative function $f(x_1, x_2) \allowbreak= a x_1 x_2$ homomorphically on puzzles $Z_1 := (u_1, v_1)$ and $Z_2 := (u_2, v_2)$, return
        $$Z = (u_1 \cdot u_2 \mod{N}, a \cdot v_1 \cdot v_2 \mod{N})$$
    \end{itemize}
    \end{construction}
%     \end{mdframed}
%     \caption{The HTLP constructions of \cite{C:MalThy19}.}
%     \label{fig:mt19-HTLP}
% \end{figure}

\noindent \Cref{con:multHTLP} operates over the solution space $\JJ_N$ (instead of $\ZZ_N$).
It is easy to see that $\htlp.\Open(\pparam,\allowbreak \htlp.{\sf Gen}(\pparam, s)) = s$ for all $s \in \ZZ_N^*$. Furthermore, for all $f(x_1, x_2) = a x_1 x_2$ and all $Z_i = (u_i, v_i) \in {\sf Im}(\htlp.{\sf Gen}(\pparam,\allowbreak s_i; r_i))$ for $i \in \{1,2\}$,
\begin{align*}
    \htlp&\rlap{$.{\sf Eval}(\pparam, f, Z_1, Z_2) = (u_1 \cdot u_2 \mod{N}, a \cdot v_1 \cdot v_2 \mod{N})$} &&\\
    &= (g^{r_1} g^{r_2} \mod{N}, &&h^{r_1} h^{r_2} \cdot a s_1 s_2 \mod{N})\\
    &= (g^{r_1 + r_2} \mod{N},   &&h^{r_1 + r_2} \cdot a s_1 s_2 \mod{N})\\
    &\rlap{$= \htlp.{\sf Gen}(\pparam, f(s_1, s_2); r_1 + r_2)$}. &&
\end{align*}
% Thus correctness holds.

% \begin{mdframed}
% \begin{construction}[Multiplicative HTLP~\cite{liu2022towards}]
% \hfill
% \begin{description}
%     \item[$\htlp.\Setup(\secparam, \Ttime) \randout \pparam$.]
%         \item[$\htlp.{\sf Gen}(\pparam, s; r) \randout Z$.] Encode the solution $s \in \mathbb{Z}_n^*$ as $m = s\chi^\sigma \in \mathbb{J}_n$, where 
%             $$\chi \in \mathbb{Z}_n^* \setminus \mathbb{J}_n,
%             \sigma = \begin{cases}
%                 0 & J_n(s) = 1\\
%                 1 & J_n(s) = -1
%             \end{cases}.$$
%     Then use the multiplicative and linear HTLPs of \cite{malavolta2019homomorphic} on $m$ and $\sigma$, respectively.
%     \item[$\htlp.\Open(\pparam, Z, r) \randout s$.]
%     \item[$\htlp.{\sf Eval}(\pparam, f, Z_1, Z_2) \randout Z$.]
% \end{description}
% \end{construction}
% \end{mdframed}

% Malavolta and Thyagarajan~\cite{C:MalThy19} construct two homomorphic time-lock puzzles (HTLPs) with, respectively, linear and multiplicative homomorphisms in groups of unknown order. For our purposes we are only interested in the former, which is based on the Paillier cryptosystem~\cite{EC:Paillier99}. It uses $N=pq$ a strong semiprime, $g \sample \ZZ_N^*$ and $h = g^{2^\Ttime}$, and has solution space $\ZZ_N$. A puzzle $Z$ is constructed as
% \begin{equation}\label{eq:paillierHTLP}
% (g^r, h^{r \cdot N} (1+N)^s) \in \mathbb{J}_N \times \ZZ_{N^2}^*
% \end{equation}
% where $\JJ_N$ is the subgroup of $\subseteq \ZZ_N^*$ of elements with Jacobi symbol +1.
% To recover $s$, a solver must recompute $h^r = (g^r)^{2^\Ttime}$, which is believed to be an inherently sequential computation in a group of unknown order.

\subsection{Vector Packing}\label{sec:packing}

\begin{definition}[Vector packing]\label{def:packing}
A setup algorithm $\PSetup$ and pair of efficiently computable bijective functions $(\pack,\unpack)$ is called a \emph{packing scheme} and has the following syntax:
    \begin{itemize}
        \item \underline{$\PSetup(\ell, w) \to \pparam$:} Given a vector dimension $\ell$ and maximum entry $w$, output public parameters $\pparam$.
        \item \underline{$\pack(\pparam, \Vec{a}) \to s$:} Encode $\Vec{a} \in (\ZZ^+)^\ell$ as a positive integer $s \in \ZZ^+$. 
        \item \underline{$\unpack(\pparam, s) \to \Vec{a}$:} Given $s \in \ZZ^+$, recover a vector $\Vec{a} \in (\ZZ^+)^\ell$. 
    \end{itemize}
For \emph{correctness} we require $\unpack(\pack(\Vec{a}))=\Vec{a}$ for all $\Vec{a}\in (\ZZ^+)^\ell$.
\end{definition}

The classic approach to packing~\cite{ACNS:Groth05,EC:HirSak00} uses a \emph{positional numeral system (PNS)} to encode a vector of entries bounded by $w$ as a single integer in base $M := w$ (see \Cref{con:packingPNS} below).
%More specifically, the vector $\vec{a}=(a_1, \dots, a_m)$ with $\forall j \in [m]:a_j < w$ is encoded as a sum of powers of $M$: the ballot contains a single integer $s := \sum_{j=1}^m a_j M^{j-1}$. Then $a_j$ can be obtained as $s \mod{M^{j-1}}$, 
Instead, we will set $M:= nw+1$ to accommodate the homomorphic addition of all $n$ users' vectors: each voter submits a length-$m$ vector with entries $\leq w$. Summing over $n$ voters, the result is a length-$m$ vector with a maximum entry value $nw$; to prevent overflow, we set $M = nw+1$.

\begin{construction}[Packing from Positional Numeral System]\label{con:packingPNS}
\hfill
\begin{itemize}%[topsep=2pt]
    \item \underline{$\PSetup(\ell, w) \to M$:} Return $M := w + 1$.
    \item \underline{$\pack(M, \Vec{a}) \to s$:} Output $s := \sum_{j=1}^{\sizeof{\vec{a}}} a_j M^{j-1}$.
    \item \underline{$\unpack(M, s) \to \Vec{a}$:} Let $\ell := \ceil{\log_M{s}}$. For $j \in [\ell]$, compute the $j$th entry of $\Vec{a}$ as $a_j := s \mod{M^{j-1}}$.
\end{itemize}
\end{construction}

Besides PNS packing, we also introduce an alternative approach in \Cref{con:packingRNS} which is based on the \emph{residue numeral system (RNS)}. The idea of the RNS packing is to interpret the entries of $\vec{a}$ as prime residues of a single unique integer $s$, which can be found efficiently using the Chinese Remainder Theorem (CRT). In other words, for all $j \in [\ell]$, $s$ captures $a_j$ as $s \bmod p_j$.

\begin{construction}[Packing from Residue Numeral System]\label{con:packingRNS}
\hfill
\begin{itemize}%[topsep=2pt]
    % \textit{Public parameters:}~$\pparam:=(p_1,\dots,p_m)$ primes s.t. $\min\limits_{j\in[m]}p_j\geq M(=nw+1)$.\\
    \item \underline{$\PSetup(\ell, w) \to \vec{p}$:} Let $M := w + 1$ and sample $\ell$ distinct primes $p_1, \dots, p_\ell$ s.t. $p_j \geq M\ \forall j \in [\ell]$. Return $\vec{p} := (p_1, \dots, p_\ell)$.
    \item \underline{$\pack(\vec{p}, \Vec{a}) \to s$:} Given $\Vec{a} \in (\ZZ^+)^\ell$, use the CRT to find the unique $s \in \ZZ^+$ s.t. $s\equiv a_j \pmod{p_j}~\forall j\in[\ell]$.
    \item \underline{$\unpack(\vec{p}, s) \to \Vec{a}$:} return $(a_1, \dots, a_\ell)$ where $a_j \equiv s \mod{p_j}\ \forall j \in [\ell]$.
\end{itemize}
\end{construction}

A major advantage of this approach is that, in contrast to the PNS approach, which is only homomorphic for SIMD (single instruction, multiple data) addition, the RNS encoding is fully SIMD homomorphic: the sum of vector encodings $\sum_{i \in [n]} s_i$ encodes the vector $\vec{a}_{+} = \sum_{i \in [n]} \vec{a}_i$, and the product $\prod_{i \in [n]} s_i$ encodes the vector $\vec{a}_{\times} = \prod_{i \in [n]} \vec{a}_i$. 

\subsection{Bilinear Pairings}\label{sec:pairings}

\begin{definition}[bilinear pairing]
   A bilinear pairing is a map $e : \GG_1 \times \GG_2 \mapsto \GG_T$ where $\GG_1, \GG_2,$ and $\GG_T$ are cyclic groups of prime order $p$. (Often, $\GG_1,\GG_2$ are written in additive notation and $\GG_T$ in multiplicative notation.) Let $g_1,h_1 \in \GG_1$ and $g_2,h_2 \in \GG_2$ (generators of their respective groups). The map $e$ has the following properties:
   \begin{description}
       \item[Bilinearity:] For all $a,b \in \ZZ_p^*$, the following hold:
       \begin{align*}
           e(g_1^a, g_2) = e(g_1, g_2^a) = e(g_1, g_2)^a\\
           e(g_1 h_1, g_2) = e(g_1, g_2) \cdot e(h_1, g_2)\\
           e(g_1, g_2 h_2) = e(g_1, g_2) \cdot e(g_1, h_2)
        \end{align*}
        \item[Non-degeneracy:] $e(g_1, g_2) \neq 1$.
        \item[Computability:] There is an efficient algorithm for computing $e$.
    \end{description}
\end{definition}

Pairings are divided into types based on the (in)equality of the utilized groups $\GG_1,\GG_2$ and whether there is an efficiently computable homomorphism between the groups. For our purposes, we will use a type-3 pairing: asymmetric ($\GG_1 \neq \GG_2$) and with no such efficiently computable homomorphism. In this case, the Decisional Diffie-Hellman (DDH) assumption is believed to hold in both $\GG_1$ and $\GG_2$; this is referred to as the \emph{symmetric external Diffie-Hellman (SXDH) assumption}.

\subsection{Shamir Secret Sharing}\label{sec:shamir}
Shamir~\cite{CACM:Shamir79} introduced a scheme to share a secret among $n$ parties such that any $t$ parties can work together to recover the secret, but with any fewer parties the secret remains information-theoretically hidden.

\begin{construction}[Shamir secret sharing \cite{CACM:Shamir79}]
Let $p$ be a prime.
    \begin{itemize}
        \item \underline{$\{s_1, \dots, s_n\} \gets \share(s, t, n)$:} Given a secret $s \in \ZZ_p$ and $t \leq n \in \mathbb{N}$, compute a $t$-out-of-$n$ sharing of $s$ by choosing a random degree-$(t-1)$ polynomial $f(X) \in \ZZ_p[X]$ such that $f(0) = s$. For $i \in [n]$, compute $s_i := (i, f(i))$.
        \item \underline{$\{s', \perp\} \gets \recon(S, t, n)$:} Given some set of shares $S$, check if $\sizeof{S} < t$. If so, output $\perp$. Otherwise, without loss of generality, let $S' := \{s_1, \dots, s_t\}$ be the first $t$ entries of $S$, where $s_i := (x_i, y_i)$. Output the Lagrange interpolation at 0:
        \[
            s' := \sum_{i=1}^t y_i \prod_{j=1, j \neq i}^t \frac{x_j}{x_j - x_i}.
        \]
    \end{itemize}
\end{construction}

The secret sharing scheme is \emph{correct}, since for any secret $s$ and values $t \leq n \in \NN$, we have $\recon(\share(s, t, n),\allowbreak t, n) = s$. 

For notational convenience, let $\share(s,t,n; r)[i]$ denote the $i$th share of $s$ computed with randomness $r$. The reconstruction algorithm can be generalized to interpolate any point $f(k)$ (not just the secret at $k=0$) and thereby recover the $i$th share:
\begin{itemize}
    \item \underline{$\{s_k,\perp\} \gets {\sf Interpolate}(S, k, t, n)$:} If $\sizeof{S} < t$, output $\perp$. Otherwise, use the first $t$ entries $(x_1, y_1), \dots,\allowbreak (x_t, y_t)$ of $S$ to interpolate
    \[
        f(k) = \sum_{i=1}^t y_i \prod_{j=1, j \neq i}^t \frac{x_j - k}{x_j - x_i}.
    \]
    Output $s_k := (k, f(k))$.
\end{itemize}

\subsection{Digital Signatures}\label{sec:signatures}

A \emph{digital signature}~\todo{\cite{todo}} is a cryptographic primitive which \todo{...}

\subsubsection{BLS Signatures}\label{sec:bls}

\begin{construction}[BLS signature scheme \cite{AC:BonLynSha01}]\label{con:bls}
Let $\GG_1, \GG_2$ be elliptic curve groups of order $p$ generated by $g_1$ and $g_2$, respectively, and $e: \GG_1 \times \GG_2 \rightarrow \GG_T$ be an efficiently computable asymmetric pairing between them. We also require a hash function $\blshash: \{0, 1\}^* \rightarrow \GG_1$. The signature scheme works as follows:
\begin{itemize}
\item \underline{$(\sk, \vk) \gets \kgen(\secparam)$:} Given the security parameter $1^\lambda$, sample $x \sample \ZZ_p$ and output the keypair consisting of signing key $\sk := x$ and verification key $\vk := g_2^x$.
\item \underline{$\sigma \gets \sign(\sk, m)$:} Given a signing key $\sk \in \ZZ_p$ and a message $m \in \{0,1\}^*$, compute a signature $\sigma := \blshash(m)^{sk}$.
\item \underline{$\{0,1\} \gets \vrfy(\vk, m, \sigma)$:} Given a verification key $\vk \in \GG_2$, message $m \in \bin^*$, and signature $\sigma \in \GG_1$, if $e(\sigma, g_2) = e(\blshash(m), \vk)$, output 1. Else output 0.
\end{itemize}
\end{construction}

The security of BLS relies on the gap co-Diffie Hellman assumption on $(\GG_1,\GG_2)$, i.e., co-DDH being easy but co-CDH being hard on $\GG_1,\GG_2$, as well as the existence of an efficiently computable homomorphism $\phi: \GG_2 \rightarrow \GG_1$ (type-2 pairing). Since we require a type-3 pairing for our purposes (i.e., no efficiently computable $\phi$ exists), we rely on a stronger variant of the co-GDH assumption (see discussion in \cite[\S3.1]{AC:BonLynSha01} and \cite[\S2.2]{EPRINT:SmaVer05}).

\paragraph{Threshold variant}
Sharing a BLS signing key $\sk \in \ZZ_p$ via Shamir secret sharing leads directly to a robust $t$-out-of-$n$ threshold signature \cite{AC:BonLynSha01}.
More specifically, each party $i \in [n]$ receives a $t$-out-of-$n$ Shamir secret share $\sk_i$ of the key. The ``partial'' public keys $\vk_i := g_2^{\sk_i}$ are published along with the public key $\vk$.

A partial signature is computed in exactly the same way as a regular BLS signature, but under the secret key share: $\sigma_i := \blshash(m)^{\sk_i}$. This value is publicly verifiable by checking that $(g_2, \vk_i, \blshash(m), \sigma_i)$ is a co-Diffie-Hellman tuple (i.e., it is of the form $(g_2, g_2^a, h, h^a)$ where $g_2 \in \GG_2$ and $h \in \GG_1$).

 Given $t$ valid partial signatures on a message $m \in \{0,1\}^*$ anyone can recover a regular BLS signature:

\begin{itemize}
    \item \underline{$\sigma \gets \recon(S := \{(i, \sigma_i)\})$}: Let $S' \subseteq S$ be the set of valid partial signatures in $S$. If $\sizeof{S'} < t$, output $\perp$. Otherwise, without loss of generality, assume the first $t$ valid signatures come from users $1, \dots, t$ and recover the complete signature as
    \[
        \sigma \gets \prod_{i=1}^t \sigma_i^{\lambda_i}, \text{ where } \lambda_i = \prod_{j=1,j\neq i}^t \frac{j}{j-i} \pmod{p}
    \]
\end{itemize}

Notice that the reconstruction simply performs Shamir reconstruction of the signing key shares $\sk_i$ in the exponent and thus the output will equal $\blshash(m)^{\sk}$. Hence, the complete signature is indistinguishable from a regular BLS signature, and verification proceeds exactly as in the regular scheme.

\subsubsection{Schnorr Signatures}\label{sec:schnorr}

\begin{construction}[Schnorr signature~\cite{C:Schnorr89}]
    Let $\pparam = (\GG, g, q)$ be description of a cyclic group $\GG$ of prime order $q$ with generator $g$ in which the discrete logarithm problem is hard.
    \begin{itemize}
        \item \underline{$\kgen(\pparam) \to (\sk, \vk)$:} Sample a secret key $\sk \sample \ZZ_q$ and set the corresponding verification key $\vk := g^{\sk} \in \GG$. Output $(\sk, \vk)$.
        \item \underline{$\sign(\sk, m) \to \sigma$:} Given a secret key $\sk \in \ZZ_q$ and message $m$, sample $k \sample \ZZ_q$ and let $R := g^k$. Compute $c := H(m, g^\sk, R)$ and $s := k + c \cdot \sk$. Output $\sigma := (R, s)$.
        \item \underline{$\vrfy(\vk, m, \sigma) \to \{0,1\}$:} Given a verification key $\vk$, message $m$, and signature $\sigma$, compute $c := H(m, \vk, R)$ and check $R \cdot \vk^c \stackrel{?}{=} s$. If so, output 1, else 0.
    \end{itemize}
\end{construction}

\noemi{Schnorr (threshold) signatures; \cite{EPRINT:Lindell22} and \cite{SAC:KomGol20} use different definitions of Schnorr, are they equivalent?}

\paragraph{Threshold variants}
Many protocols exist for threshold Schnorr signatures~\todo{\cite{todo}}, each offering a different combination of properties. \todo{...}

The BLS threshold signature scheme~\cite{AC:BonLynSha01} is fully non-interactive, meaning that signing occurs in one round.
FROST is a ``somewhat'' non-interactive threshold signature scheme~\cite{EPRINT:BelTesZhu22} in the sense that the \emph{signing} procedure consists of a single round, but there is another round of message-independent pre-processing.     

\subsection{KZG polynomial commitments}\label{sec:kzg}

KZG commitments can be instantiated using either a symmetric or asymmetric pairing; we give the asymmetric version of KZG below.

\begin{construction}[KZG polynomial commitments~\cite{AC:KatZavGol10}]
Let $\GG_1, \GG_2$ be elliptic curve groups of order $p$ with generators $g_1,g_2$ and $e: \GG_1 \times \GG_2 \mapsto \GG_T$ be an elliptic curve pairing. The following is a commitment scheme for polynomials in $\ZZ_p[X]$ with degree at most $d$.
\begin{itemize}
    \item \underline{$\crs \gets \setup(d):$} Sample $\tau \sample \ZZ_p$ and output $\crs := \{g_1, g_1^\tau, g_1^{\tau^2}, \dots, g_1^{\tau^d},\allowbreak g_2, g_2^\tau\}$.
    \item \underline{$\com_f \gets \Commit(\crs, f(X)):$} Let $f(X) = a_0 + a_1 X + \dots + a_d X^d \in \ZZ_p[X]$. Use $\crs$ to compute and output $g_1^{f(\tau)} = g_1^{a_0} \cdot (g_1^\tau)^{a_1} \dots (g_1^{\tau^d})^{a_d} = g_1^{a_0 + a_1 \tau + \dots + a_d \tau^d}\allowbreak \in \GG_1$.
    \item \underline{$(f(i), \pi_i) \gets \Open(\crs, f(X), i):$} To open $f(X)$ at $i$, let $q_i(X) := \frac{f(X) - f(i)}{X - i}\allowbreak \in \ZZ_p[X]$\footnote{This is a polynomial by Little BÃ©zout's Theorem.}. Then compute $\com_{q_i} \gets \Commit(\crs, q_i(X))$ and output $(f(i),\allowbreak \com_{q_i}) \in \ZZ_p \times \GG_1$.
    \item \underline{$\{0,1\} \gets \vrfy(\crs, \com_f, i, y, \pi_i):$} To confirm $y = f(i)$, it suffices to check that $q_i(X) = \frac{f(X) - y}{X - i}$ at $X=\tau$. This can be done with a single pairing check:
    \[
        e(\com_f / g_1^y, g_2) \stackrel{?}{=} e(\pi_i, g_2^\tau / g_2^i)
    \]
\end{itemize}
\end{construction}

% Notice that this scheme is linearly homomorphic: given commitments $\com_f, \com_{f'}$ to two distinct polynomials $f(X),f'(X)$, the commitment to $(f+f')(X)$ is $\com_{f+f'} := \com_f \cdot \com_{f'}$. The same holds with the corresponding evaluation proofs $\pi_{f,i}, \pi_{f',i}$ at some point $i$: the evaluation proof for the point $(f+f')(i)$ is $\pi_{f+f',i} := \pi_{f,i} \cdot \pi_{f',i}$. Scaling the polynomial $f(X)$ by a constant $c \in \ZZ_p$ is also simple: the new commitment is $\com_{cf} := \com_f^c$ and an adjusted evaluation proof is obtained as $\pi_{cf, i} := \pi_{f,i}^c$.

The security of the scheme relies on the $d$-Strong Diffie Hellman assumption ($d$-SDH), which states that given $\{g_1, g_1^\tau, \dots, g_1^{\tau^d}, g_2, g_2^\tau\}$, it is difficult to compute $(c, g_1^{\frac{1}{\tau-c}})$ for any $c \in \ZZ_p \setminus \{-\tau\}$. This assumption is stronger than $d$-SDH in $\GG_1$, which in turn implies DDH in $\GG_1$.

\subsection{Pedersen Commitments}\label{sec:pedersen}

Next, we recall Pedersen commitments~\cite{C:Pedersen91}, a commitment scheme which is unconditionally (information-theroetically) hiding and computationally binding (by the discrete logarithm assumption on $\GG$). In our construction we will instantiate the scheme over $\GG_1$, so we let $\GG = \GG_1$ in the description below.

\begin{construction}[Pedersen commitment scheme~\cite{C:Pedersen91}]
Let $\GG_1$ be a group of order $p$ and $g_1,h_1$ be generators of $\GG_1$. The following is a commitment scheme for elements $x \in \ZZ_p$.
\begin{itemize}
    \item \underline{$(\com, \decom) \gets \Commit(x):$} Sample $r \sample \ZZ_p$ and return $\com := g_1^x h_1^r$ and decommitment information $(x, r)$.
    \item \underline{$(x, r) \gets \Open(\com, \decom):$} To open $\com$, directly output $\decom = (x, r)$.
    \item \underline{$\{0,1\} \gets \vrfy(\com, x, r):$} To confirm the opening of $\com$ to $x$, it suffices to check that $\com = g_1^x h_1^r$.
\end{itemize}
\end{construction}

A PoK of the committed value can be computed using a Sigma protocol due to Okamoto~\cite{C:Okamoto92}, which can be made non-interactive using the Fiat-Shamir transform~\cite{C:FiaSha86}. We refer to this protocol as $\Pi_{\sf ped}$ and present it in \Cref{fig:pi_ped}.

\begin{figure*}[tbh]
   \centering
   \begin{mdframed}
   \begin{center}
       \textsc{PoK of Pedersen opening ($\Pi_{\sf ped}$)}
   \end{center}
   \medskip
   \textbf{Parameters:} Group $\GG_1$ of order $p$ with generators $g_1, h_1$.
   \hfill\medskip\\
   \underline{$\mathsf{Prove}(\com_{\sf ped}; (v, r)) \to \pi_{\sf ped}$:} Given a Pedersen commitment $\com_{\sf ped} = g_1^v h_1^r$, a party can prove knowledge of the opening $(v,r)$ as follows:
   \begin{enumerate}
       \item The prover samples $s_1, s_2 \sample \ZZ_p$ and sends $a := g_1^{s_1} h_1^{s_2}$ to the verifier.
       \item The verifier sends back a uniform challenge $c \sample \ZZ_p$.
       \item The prover computes $t_1 := s_1 + vc$ and $t_2 := s_2 + rc$ and sends both values to the verifier.
   \end{enumerate}
   The proof is defined as $\pi_{\sf ped} := (a, c, (t_1, t_2))$.\medskip\\
   \underline{$\vrfy(\com_{\sf ped}, \pi_{\sf ped}) \to \{0,1\}$:} Given the commitment $\com_{\sf ped}$ and a proof $\pi_{\sf ped}$, the verifier parses $\pi_{\sf ped} := (a, c, (t_1, t_2))$ and outputs 1 iff $a \cdot \com_{\sf ped}^c = g_1^{t_1} h_1^{t_2}$.
   \end{mdframed}
   \caption{The proof system $\Pi_{\sf ped}$ used to prove knowledge of the opening to a Pedersen commitment~\cite{C:Okamoto92}.}
   \label{fig:pi_ped}
\end{figure*}

\subsection{Leftover Hash Lemma}\label{sec:lhl}  

We use the presentation of the leftover hash lemma (LHL)~\cite{FOCS:ImpZuc89} from\break \cite{EC:AMPR19}.\footnote{We specifically use the improved version from the Journal of Cryptology version of this paper.}  Let $\left(\mathcal{X},\oplus\right)$ be a finite group of size $\left|\mathcal{X}\right|$, and let $n$ be a positive integer. For any fixed $2n$-vector of group elements $\mathbf{x} = \left\lbrace x_{j,b} \right\rbrace_{j\in \left[ n \right],b\in\left\lbrace 0,1 \right\rbrace} \in\mathcal{X}^{2n}$, denote by $\mathcal{S}_{\mathbf{x}}$ the following distribution:
\begin{equation}
	\mathcal{S}_{\mathbf{x}} = \Big\{\bigoplus_{j\in[n]}x_{j,r_j} : \left(r_1,\cdots,r_n\right)\gets\left\lbrace 0,1 \right\rbrace^n\Big\}.\nonumber
\end{equation}
Also, let $\mathcal{U}_{\mathcal{X}}$ denote the uniform distribution over $\mathcal{X}$, and let $\Delta \left(\mathcal{D}_1,\mathcal{D}_2\right)$ denote the statistical distance between the distributions $\mathcal{D}_1$ and $\mathcal{D}_2$. We will use the following special case of leftover hash lemma~\cite{FOCS:ImpZuc89}. The proof can be found in the JoC version of~\cite{EC:AMPR19}.

\begin{lemma}{\textnormal{(Leftover Hash Lemma.)}}
	\label{lemma:LHL}	
	Let $\left(\mathcal{X},\oplus\right)$ be a finite group, and let $\mathcal{S}_{\mathbf{x}}$ and $\mathcal{U}_{\mathcal{X}}$ be two distributions over $\mathcal{X}$ as defined above. For any (large enough) positive integer $n$, it holds that
	\begin{equation}
		\Pr_{\mathbf{x}\gets\mathcal{X}^{2n}}\left[\Delta\left(\mathcal{S}_{\mathbf{x}},\mathcal{U}_{\mathcal{X}}\right)> \sqrt[4]{\frac{\sizeof{\mathcal{X}}}{2^n}}  \right]\leq \sqrt[4]{\frac{\sizeof{\mathcal{X}}}{2^n}} \nonumber.
	\end{equation}
	In particular, for any $n > \log(\sizeof{\mathcal{X}}) + \omega(\log(\lambda))$, if $\mathbf{x}$ is sampled uniformly then with overwhelming probability the statistical distance between two distributions is negligible.
\end{lemma}

\subsection{Universal Composability (UC) Framework}\label{sec:uc}

In the universal composability (UC) framework~\cite{FOCS:Canetti01}, the security requirements of a protocol are defined via an \emph{ideal functionality} which is executed by a trusted party. To prove that a protocol \emph{UC-realizes} a given ideal functionality, we show that the execution of this protocol (in the real or hybrid world) can be \emph{emulated} in the ideal world, where in both worlds there is an additional adversary $\env$ (the environment) which models arbitrary concurrent protocol executions. Specifically, we show that for any adversary $\adv$ attacking the protocol execution in the real world (by controlling communication channels and corrupting parties involved in the protocol execution), there exists an adversary $\Sim$ (the simulator) in the ideal world who can produce a protocol execution which no environment $\env$ can distinguish from the real-world execution.

Below we describe the UC framework as it is presented in \cite{EPRINT:CLOS02}. 
All parties are represented as probabilistic interactive Turing machines (ITMs) with input, output, and ingoing/outgoing communication tapes. For simplicity, we assume that all communication is authenticated, so an adversary can only delay but not forge or modify messages from parties involved in the protocol. Therefore, the order of message delivery is also not guaranteed (asynchronous communication). We consider a PPT malicious, adaptive adversary who can corrupt or tamper with parties at any point during the protocol execution.
%(modeled in the ideal world via the interfaces in \Cref{fig:FSign3}).

The execution in both worlds consists of a series of sequential party activations. Only one party can be activated at a time (by writing a message on its input tape). In the real world, the execution of a protocol $\Pi$ occurs among parties $P_1, \dots, P_n$ with adversary $\adv$ and environment $\env$. In the ideal world, interaction takes place between dummy parties $\tilde{P}_1, \dots, \tilde{P}_n$ communicating with the ideal functionality $\F$, with the adversary (simulator) $\Sim$ and environment $\env$. Every copy of $\F$ is identified by a unique session identifier $\texttt{sid}$. 
% The environment $\env$ can read all parties' output tapes and write to any party's input tape, including the adversary $\adv$ or $\Sim$. $\adv$ can read the outgoing communication tapes of all parties $P_i$ and \emph{deliver} messages between parties by writing them on the corresponding incoming communication tape (this models the asynchronous but authenticated communication). $\adv$ can also \emph{corrupt} any party $P_i$, and $\env$ will be notified. In the ideal world, the dummy parties simply copy any input they receive to their outgoing communication tape. $\Sim$ interacts primarily with the ideal functionality $\F$ in several ways: it can \emph{read} the public headers of any messages on its incoming and and outgoing communication tapes, \emph{write} on $\F$'s incoming communication tape, or \emph{deliver} messages from $\F$ to a dummy party or vice versa. It can also \emph{corrupt} any dummy party $\tilde{P}_i$, and $\env$ and $\F$ will be notified.

In both the real and ideal worlds, the environment is activated first and activates either the adversary ($\adv$ resp. $\Sim$) or an uncorrupted (dummy) party by writing on its input tape. If $\adv$ (resp. $\Sim$) is activated, it can take an action or return control to $\env$. After a (dummy) party (or $\F$) is activated, control returns to $\env$. The protocol execution ends when $\env$ completes an activation without writing on the input tape of another party.

We denote with $\real_{\Pi,\adv,\env}(\secpar,x)$ the random variable describing the output of the real-world execution of $\Pi$ with security parameter $\secpar$ and input $x$ in the presence of adversary $\adv$ and environment $\env$. We write the corresponding distribution ensemble as $\{ \real_{\Pi,\adv,\env}(\secpar, x) \}_{\secpar \in \mathbb{N}, x \in \{0,1\}^*}$. The output of the ideal-world interaction with ideal functionality $\F$, adversary (simulator) $\Sim$, and environment $\env$ is represented by the random variable $\ideal_{\F,\Sim,\env}(\secpar, x)$ and corresponding distribution ensemble $\{ \ideal_{\F,\Sim,\env}(\secpar, x) \}_{\secpar \in \mathbb{N}, x \in \{0,1\}^*}$.

The actions each party can take are summarized below:
\begin{itemize}
   \item Environment $\env$: \textbf{read} output tapes of the adversary ($\adv$ or $\Sim$) and any uncorrupted (dummy) parties; then \textbf{write} on the input tape of one party (the adversary $\adv$ or $\Sim$ or any uncorrupted (dummy) parties).
   \item Adversary $\adv$: \textbf{read} its own tapes and the outgoing communication tapes of all parties; then \textbf{deliver} a pending message to party by writing it on the recipient's ingoing communication tape \emph{or} \textbf{corrupt} a party (which becomes inactive: its tapes are given to $\adv$ and $\adv$ controls its actions from this point on, and $\env$ is notified of the corruption).
   \item Real-world party $P_i$: only follows its code (potentially writing to its output tape or sending messages via its outgoing communication tape).
   \item Dummy party $\tilde{P}_i$: acts only as a simple relay with the ideal functionality $\F$, copying inputs from its input tape to its outgoing communication tape (to $\F$) and any messages received on its ingoing communication tape (from $\F$) to its output tape.
   \item Adversary $\Sim$: \textbf{read} its own input tape and the public headers (see below) of the messages on $\F$'s and dummy parties' outgoing communication tapes; then \textbf{deliver} a message to $\F$ from a dummy party or vice versa by copying it from the sender's outgoing communication tape to the recipient's incoming communication tape \emph{or} \textbf{send} its own message to $\F$ by writing on the latter's incoming communication tape \emph{or} \textbf{corrupt} a dummy party (which becomes inactive: its tapes are given to $\Sim$ and $\Sim$ controls its actions from this point on, and $\env$ and $\F$ are notified of the corruption).
   \item Ideal functionality $\F$: \textbf{read} incoming communication tape; then \textbf{send} any messages specified by its definition to the dummy parties and/or adversary $\Sim$ by writing to its outgoing communication tape.
\end{itemize}

\begin{definition}
    We say a protocol $\Pi$ \emph{UC-realizes} an ideal functionality $\F$ if for any PPT adversary $\adv$, there exists a simulator $\Sim$ such that for any environment $\env$, the distribution ensembles $\{ \real_{\Pi,\adv,\env}(\secpar, x) \}_{\secpar \in \mathbb{N}, x \in \bin^*}$ and $\{ \ideal_{\F,\Sim,\env}(\secpar,\allowbreak x) \}_{\secpar \in \mathbb{N},x \in \bin^*}$ are computationally indistinguishable.
\end{definition}